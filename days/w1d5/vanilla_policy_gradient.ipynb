{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d5/vanilla_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "## Vanilla Policy Optimisation\n",
    "\n",
    "We will be looking at an implementation of the vanilla policy gradient algorithm, to train a policy to play CartPole: the goal is to balance a stick on a sliding cart. The agent can move the cart left or right. The episode ends when the stick falls over too much, or the cart moves too far away from the center.\n",
    "\n",
    "![CartPole](https://pytorch.org/tutorials/_images/cartpole.gif)\n",
    "\n",
    "\n",
    "Instructions:\n",
    "- Answer the questions\n",
    "- Read all the code\n",
    "- Complete the Typing in the function get_action\n",
    "- Complete the compute_loss function.\n",
    "- Run the script with the defaults parameters on the terminal\n",
    "- Find a set of hyperparameters to reach ep_len : 200 steadily.\n",
    "\n",
    "\n",
    "Don't begin working on this algorithms if you don't understand the first theorem in this blog: https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html\n",
    "\n",
    "This exercise is short, but you should aim to understand everything in this code. Simply completing the types is not sufficient. The important thing here is to have a good understanding of each line of code, as well as the policy gradient theorem that we are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "- Is vanilla policy gradient (VPG) model based or model free?\n",
    "- Is VPG on-policy or off-policy?\n",
    "- Should the loss decrease during the training, why?\n",
    "\n",
    "Answers:\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jaxtyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "\n",
    "from jaxtyping import Array, Float, Int\n",
    "\n",
    "\n",
    "def mlp(sizes, activation=nn.Tanh, output_activation=nn.Identity):\n",
    "    # Build a feedforward neural network.\n",
    "    layers = []\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    \n",
    "    # What does * mean here? Search for unpacking in python\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
    "          epochs=50, batch_size=5000, render=False):\n",
    "\n",
    "    # make environment, check spaces, get obs / act dims\n",
    "    env = gym.make(env_name)\n",
    "    assert isinstance(env.observation_space, Box), \\\n",
    "        \"This example only works for envs with continuous state spaces.\"\n",
    "    assert isinstance(env.action_space, Discrete), \\\n",
    "        \"This example only works for envs with discrete action spaces.\"\n",
    "\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_acts = env.action_space.n\n",
    "\n",
    "    # Core of policy network\n",
    "    # What should be the sizes of the layers of the policy network?\n",
    "    logits_net = mlp(sizes=[obs_dim]+hidden_sizes+[n_acts])\n",
    "\n",
    "\n",
    "    def get_policy(obs: Float[Array, \"*b obs_dim\"]):\n",
    "        \"\"\"Make function to compute action distribution with or without batch.\"\"\"\n",
    "        logits = logits_net(obs)\n",
    "        # Tip: Categorical is a convenient pytorch object which enable register logits (or a batch of logits)\n",
    "        # and then being able to sample from this pseudo-probability distribution with the \".sample()\" method.\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def get_action(obs): # To be typed\n",
    "        \"\"\"Make action selection function (outputs int actions, sampled from policy)\"\"\"\n",
    "        return get_policy(obs.unsqueeze(0)).sample().item()\n",
    "\n",
    "    # make loss function whose gradient, for the right data, is policy gradient\n",
    "    # What is the shape of obs?\n",
    "    def compute_loss(obs: Float[Array, \"b obs_dim\"], acts: Int[Array, \"b\"], rewards: Float[Array, \"b\"]):\n",
    "        \"\"\"TODO\"\"\"\n",
    "        # rewards: a piecewise constant vector containing the total reward of each episode.\n",
    "        \n",
    "        # Use the get_policy function to get the categorical object, then sample from it with the 'log_prob' method.\n",
    "        ...\n",
    "\n",
    "    # make optimizer\n",
    "    optimizer = Adam(logits_net.parameters(), lr=lr)\n",
    "\n",
    "    # for training policy\n",
    "    def train_one_epoch():\n",
    "        # make some empty lists for logging.\n",
    "        batch_obs = []          # for observations\n",
    "        batch_acts = []         # for actions\n",
    "        batch_weights = []      # for R(tau) weighting in policy gradient\n",
    "        batch_rets = []         # for measuring episode returns # What is the return?\n",
    "        batch_lens = []         # for measuring episode lengths\n",
    "\n",
    "        # reset episode-specific variables\n",
    "        obs = env.reset()       # first obs comes from starting distribution \n",
    "        done = False            # signal from environment that episode is over\n",
    "        ep_rews = []            # list for rewards accrued throughout ep\n",
    "\n",
    "        # render first episode of each epoch\n",
    "        finished_rendering_this_epoch = False\n",
    "\n",
    "        # collect experience by acting in the environment with current policy\n",
    "        while True:\n",
    "\n",
    "            # rendering\n",
    "            if (not finished_rendering_this_epoch) and render:\n",
    "                env.render()\n",
    "\n",
    "            # save obs\n",
    "            batch_obs.append(obs.copy())\n",
    "\n",
    "            # act in the environment\n",
    "            act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            obs, rew, done, _ = env.step(act)\n",
    "\n",
    "            # save action, reward\n",
    "            batch_acts.append(act)\n",
    "            ep_rews.append(rew)\n",
    "\n",
    "            if done:\n",
    "                # if episode is over, record info about episode\n",
    "                # Is the reward discounted?\n",
    "                ep_ret, ep_len = sum(ep_rews), len(ep_rews)\n",
    "                batch_rets.append(ep_ret)\n",
    "                batch_lens.append(ep_len)\n",
    "\n",
    "                # the weight for each logprob(a|s) is R(tau)\n",
    "                # Why do we use a constant vector here?\n",
    "                batch_weights += [ep_ret] * ep_len\n",
    "\n",
    "                # reset episode-specific variables\n",
    "                obs, done, ep_rews = env.reset(), False, []\n",
    "\n",
    "                # won't render again this epoch\n",
    "                finished_rendering_this_epoch = True\n",
    "\n",
    "                # end experience loop if we have enough of it\n",
    "                if len(batch_obs) > batch_size:\n",
    "                    break\n",
    "\n",
    "        # take a single policy gradient update step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_loss = compute_loss(obs=torch.as_tensor(batch_obs, dtype=torch.float32),\n",
    "                                  acts=torch.as_tensor(batch_acts, dtype=torch.int32),\n",
    "                                  rewards=torch.as_tensor(batch_weights, dtype=torch.float32)\n",
    "                                  )\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        return batch_loss, batch_rets, batch_lens\n",
    "\n",
    "    # training loop\n",
    "    for i in range(epochs):\n",
    "        batch_loss, batch_rets, batch_lens = train_one_epoch()\n",
    "        print('epoch: %3d \\t loss: %.3f \\t return: %.3f \\t ep_len: %.3f'%\n",
    "                (i, batch_loss, np.mean(batch_rets), np.mean(batch_lens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(env_name='CartPole-v0', hidden_sizes=[32], lr=1e-2, \n",
    "          epochs=50, batch_size=50, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original algo here: https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/vpg/vpg.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML4G')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92168008c1991bbe6f37c0a293534e68a58b2d6f9d0a850eaf02432aa4f31239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
