{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/days/w1d2/hyperparameters/TP1_Ex1_Binary_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Binary classification problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset\n",
    "\n",
    "We study first a binary classification problem, performed by a neural network. Each input has two real features, that is, they are points in 2D and the output can be only 0 or 1.\n",
    "\n",
    "The training set contains 4000 examples, and the validation set, 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Display figures on jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate the dataset, in the form of two interlaced spirals\n",
    "def spiral(phi):\n",
    "    x = (phi + 1) * torch.cos(phi)\n",
    "    y = phi * torch.sin(phi)\n",
    "    return torch.cat((x, y), dim=1)\n",
    "\n",
    "\n",
    "def generate_data(num_data):\n",
    "    angles = torch.empty((num_data, 1)).uniform_(1, 15)\n",
    "    data = spiral(angles)\n",
    "    # add some noise to the data\n",
    "    data += torch.empty((num_data, 2)).normal_(0.0, 0.4)\n",
    "    labels = torch.zeros((num_data,), dtype=torch.int)\n",
    "    # flip half of the points to create two classes\n",
    "    data[num_data // 2 :, :] *= -1\n",
    "    labels[num_data // 2 :] = 1\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training set with 4000 examples\n",
    "x_train, y_train = generate_data(4000)\n",
    "\n",
    "print(\"X_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(x, y):\n",
    "    \"\"\"Plot labeled data points X and y. Label 1 is a red +, label 0 is a blue +.\"\"\"\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(x[y == 1, 0], x[y == 1, 1], \"r+\")\n",
    "    plt.plot(x[y == 0, 0], x[y == 0, 1], \"b+\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now invoke the `plot_data` function on the dataset previously generated to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `TensorDataset` wrapper from pytorch, so that the framework can easily understand our tensors as a proper dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "training_set = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  A neural network to classify the data\n",
    "\n",
    "Here is a skeleton of a neural network with, by default, a single hidden layer. This is the model you'll try to improve during this exercise.\n",
    "\n",
    "Look at the code and run it to see the structure, then follow the questions below to iteratively improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the first step, we define a neural network with just two layers. A useful tutorial for constructing model can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"\n",
    "    A fully connected neural network with any number of layers.\n",
    "    \"\"\"\n",
    "\n",
    "    NAME_TO_NONLINEARITY = {\n",
    "        \"relu\": nn.ReLU,\n",
    "        \"sigmoid\": nn.Sigmoid,\n",
    "        \"tanh\": nn.Tanh,\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self, layers=[2, 10, 1], non_linearity: Literal[\"relu\", \"sigmoid\", \"tanh\"] = \"relu\"\n",
    "    ):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        modules = []\n",
    "        for input_dim, output_dim in zip(layers[:-1], layers[1:]):\n",
    "            modules.append(nn.Linear(input_dim, output_dim))\n",
    "            # After each linear layer, we apply a non-linearity\n",
    "            modules.append(self.NAME_TO_NONLINEARITY[non_linearity]())\n",
    "\n",
    "        # Remove the last non-linearity, since the last layer is the output layer\n",
    "        self.layers = nn.Sequential(*modules[:-1])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        ouput = self.layers(inputs)\n",
    "\n",
    "        # We want the model to predict 0 for one class and 1 for the other class\n",
    "        # A Sigmoid activation function appropriate to map the output from [-inf, inf] to [0, 1]\n",
    "        prediction = torch.sigmoid(ouput)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "model = Model()\n",
    "\n",
    "# Choose the hyperparameters for training:\n",
    "num_epochs = 40\n",
    "batch_size = 10\n",
    "\n",
    "# Training criterion. This one is a mean squared error (MSE) loss between the output\n",
    "# of the network and the target label\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Use SGD optimizer with a learning rate (lr) of 0.01\n",
    "# It is initialized on our model\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "More information can be found [here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tqdm is a library used to display progress bars. It's extremely useful when training.\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def train(num_epochs: int, batch_size: int, criterion, optimizer, model, dataset, verbose: bool = False):\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    # Store the training errors\n",
    "    train_losses = []\n",
    "    # Create a DataLoader to iterate over the dataset in batches\n",
    "    train_loader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_average_loss = 0\n",
    "        # Each epoch, we iterate over the dataset once\n",
    "        for x_batch, y_true in train_loader:\n",
    "            # Compute the predictions.\n",
    "            # Output shape is (batch_size, 1), so we squeeze the last dimension\n",
    "            y_predicted = model(x_batch).squeeze(1)\n",
    "\n",
    "            # The loss is how far the predictions are from the true labels\n",
    "            loss = criterion(y_predicted, y_true.float())\n",
    "\n",
    "            # Do gradient descent to minimize the loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record the average loss for this batch\n",
    "            epoch_average_loss += loss.item() * batch_size / len(dataset)\n",
    "\n",
    "        train_losses.append(epoch_average_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_average_loss:.4f}\")\n",
    "\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = train(num_epochs, batch_size, criterion, optimizer, model, training_set, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error wrt. the number of epochs\n",
    "plt.plot(range(1, num_epochs + 1), train_losses)\n",
    "plt.xlabel(\"num_epochs\")\n",
    "plt.ylabel(\"Train error\")\n",
    "plt.title(\"Visualization of convergence\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model on the validation set\n",
    "\n",
    "We first evaluate the accuracy on a validation set, to see how the model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 validation datapoints\n",
    "x_val, y_val = generate_data(1000)\n",
    "\n",
    "\n",
    "def get_accuracy(model, x=x_val, y=y_val):\n",
    "    \"\"\"Compute the accuracy of the model on a dataset.\"\"\"\n",
    "    # Compute the predictions, without keeping track of the gradients\n",
    "    with torch.no_grad():\n",
    "        y_predicted = model(x).squeeze(1)\n",
    "\n",
    "    # The predictions are in [0, 1] and the labels are either 0 or 1\n",
    "    # So we round the predictions to get the predicted labels\n",
    "    y_predicted = torch.round(y_predicted)\n",
    "\n",
    "    # Compute the accuracy by counting the number of correct predictions\n",
    "    accuracy = (y_predicted == y).sum().item() / len(y)\n",
    "\n",
    "    print(f\"Accuracy on {len(y)} examples: {accuracy:.2%}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_accuracy(model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visualize what the model has learned by plotting all the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(model, x=x_val, y_real=y_val):\n",
    "    \"\"\"Compare the prediction with real labels.\"\"\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_predicted = model(x).squeeze(1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    reds = y_real > 0.5\n",
    "    plt.subplot(121)\n",
    "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
    "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
    "    plt.title(\"real data\")\n",
    "\n",
    "    reds = y_predicted > 0.5\n",
    "    plt.subplot(122)\n",
    "    plt.plot(x[reds, 0], x[reds, 1], \"r+\")\n",
    "    plt.plot(x[~reds, 0], x[~reds, 1], \"b+\")\n",
    "    plt.title(\"predicted data\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_predictions(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-optimisation\n",
    "\n",
    "We defined a lot a hyper-parameters (learning rate, layer sizes...) in the previous section. We will now try to find the best combination of hyper-parameters.\n",
    "\n",
    "> WARNING: For this exercise to be maximally useful, before you start answering the questions, try to make predictions about the impact of each meta-parameter.\n",
    "Afterwards, you can check that the predictions were correct.\n",
    "\n",
    "Bonus: if you want, you can make your predictions on [FateBook](https://fatebook.io).\n",
    "\n",
    "\n",
    "Questions:\n",
    "- Meta optimization: The goal of this tutorial is to get a summary table of the impact of each meta-parameter by clicking once on the \"Run All\" button.\n",
    "To do this, you need to think about the difference between the grid search strategy and the sensitivity analysis strategy? Which strategy is more suitable in case there are a lot of meta parameters? Try to implement this strategy in the following.\n",
    "- Why do you need the test dataset in addition to the validation dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Impact of the architecture of the model\n",
    "\n",
    "The class `Model` is the definition of your model.\n",
    "Retrain the model by using different architectures, you can change them in the previous sections definition, or put everything you need in the cell below for convenience.\n",
    "\n",
    "Try out different architectures and\n",
    "see the impact of the following factors:\n",
    "\n",
    "* Try to add more layers (1, 2, 3, more ?)\n",
    "* Try to different activation functions ([sigmoid](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.sigmoid), [tanh](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.tanh), [relu](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.relu), etc.)\n",
    "* Try to change the number of neurons for each layer (5, 10, 20, more ?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Impact of the optimizer\n",
    "\n",
    "* Use different batch size from 10 to 400\n",
    "* Try different values of the learning rate (between 0.001 and 10), and see how these impact the training process. Do all network architectures react the same way to different learning rates?\n",
    "* Change the duration of the training by increasing the number of epochs\n",
    "* Try other optimizers, such as [Adam](https://pytorch.org/docs/stable/optim.html?highlight=adam#torch.optim.Adam) or [RMSprop](https://pytorch.org/docs/stable/optim.html?highlight=rmsprop#torch.optim.RMSprop)\n",
    "\n",
    "**Note:** These changes may interact with your previous choices of architectures, and you may need to change them as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Impact of the loss function\n",
    "\n",
    "The current model uses a mean square error (MSE) loss. While this loss can be used in this case, it is now rarely used for classification, and instead a Binary Cross Entropy (BCE) is used. It consists in interpreting the output of the network as the probability $p(y | x)$ of the point $x$ to belong to the class $y$, and in maximizing the probability to be correct for all samples $x$, that is, in maximizing $\\displaystyle \\prod_{(x,y) \\in Dataset} p(y|x)$. Applying $-\\log$ to this quantity, we obtain the following criterion to minimize:\n",
    "\n",
    "$$ \\sum_{(x,y) \\in Dataset} - \\log p(y | x) $$\n",
    "\n",
    "This is implemented as such by the [BCELoss](https://pytorch.org/docs/stable/nn.html?highlight=bce#torch.nn.BCELoss) of pytorch. Note that this criterion requires its input to be a probability, i.e. in $[0,1]$, which requires the use of an appropriate activation function beforehand, e.g., a sigmoid.\n",
    "\n",
    "It turns out that, for numerical stability reasons, it is better to incorporate this sigmoid and the BCELoss into a single function; this is done by the [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html?highlight=bcewithlogit#torch.nn.BCEWithLogitsLoss). Try to replace the MSE by this one and see how this changes the behavior in the network. This can also interact with the changes of the two previous exercices.\n",
    "\n",
    "**Note:** As a consequence, when using the BCEWithLogitsLoss, the last layer of your network should not be followed by an activation function, as BCEWithLogitsLoss already adds a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Prediction on test set\n",
    "\n",
    "Once you have a model that seems satisfying on the validation dataset, you SHOULD evaluate it on a test dataset that has never been used before, to obtain a final accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ML4G')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "92168008c1991bbe6f37c0a293534e68a58b2d6f9d0a850eaf02432aa4f31239"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
