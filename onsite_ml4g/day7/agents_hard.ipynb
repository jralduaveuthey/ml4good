{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G-2.0/blob/master/workshops/agents/agents_hard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# LLM Agents\n",
    "\n",
    "This notebook is an introduction to the openai & anthropics API and to the design of LLM-agents.\n",
    "\n",
    "In the first part, your goal will be to make a chatbot that negotiates the price of a specific good with you, then against an other model. This could be part of a persuation benchmark, where we evaluate how well a LLM can drive the price down against an other LLM.\n",
    "\n",
    "\n",
    "> ## Learning outcomes\n",
    "> - Knowing how to use LLM API\n",
    "> - Finding your way in the documentation\n",
    "> - Understand how to make LLMs take actions\n",
    "> - Experiment with prompt engineering and control LLM outputs\n",
    "\n",
    "During the workshop, you will need the documentation \n",
    "- For the OpenAI API: https://platform.openai.com/docs/\n",
    "- For the Anthropics API: https://docs.anthropic.com/claude/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "except ImportError:\n",
    "    pass\n",
    "else:  # In colab\n",
    "    %pip install openai anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "import anthropic\n",
    "import tomllib\n",
    "from typing import Callable\n",
    "\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\") or input(\"OpenAI API Key\")\n",
    "anthropic_key = os.environ.get(\"ANTHROPIC_API_KEY\") or input(\"Anthropic API Key\")\n",
    "\n",
    "openai_client = openai.Client(api_key=openai_key)\n",
    "anthropic_client = anthropic.Client(api_key=anthropic_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\n",
    "    # Small, cheap and fast\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    # Medium\n",
    "    \"gpt-3.5-turbo\",\n",
    "    \"claude-3-sonnet-20240229\",\n",
    "    # Big, slow expensive and good\n",
    "    \"gpt-4-turbo-preview\",\n",
    "    \"claude-3-opus-20240229\",\n",
    "]\n",
    "\n",
    "CLAUDE_SMALL, GPT3, CLAUDE_MEDIUM, GPT4, CLAUDE_BIG = MODELS\n",
    "MODEL = MODELS[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding OpenAI's API\n",
    "\n",
    "According to their quickstart guide, the following is an example of how to use the API. \n",
    "Try to understand what each parameter does, change them and see what happens.\n",
    "\n",
    "<details>\n",
    "<summary>Why is the messages parameter a list? What are each of its elements?</summary>\n",
    "\n",
    "`message` is a list of each message in a conversation. They correspond to one chat, with messages from the assistant and the user, as you would see in the ChatGPT interface. Under the hood, the API concatenates them, and include markers tokens to diferenciate between the roles of `\"user\"`, `\"assistant\"` and `\"system\"`.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai_client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a poetic assistant, skilled in explaining complex programming concepts with creative flair.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Compose a poem that explains the concept of recursion in programming.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat human-LLM\n",
    "\n",
    "We'll start be creating one function to handle all the details of the APIs, so that we can forget about them later and focus more on the logic.\n",
    "\n",
    "**Important note**: When you develop applications, evaluations or benchmarks with LLMs it is important always test with the smallest model first, as they are much faster and cheaper. This let you do more and faster iterations. However, when you start to tweak prompts, you need to tweak your prompts for one specific LLM, as they all react differently. The best prompt on GPT3 can be quite bad on GPT4 and vice versa.\n",
    "\n",
    "<!-- Start by having the function work for openai's models, test it on the cells bellow, and you can later come back and implement it for anthropic. The anthropic part is especially interesting when we get to make the two of them chat. Who's the most persuasive? -->\n",
    "We have the function already implemented for anthropic, and it should work once you fill `message_dicts` with the correct format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(system: str, *messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"\n",
    "    Generate the next message from the specified model.\n",
    "\n",
    "    Args:\n",
    "        system: the system prompt to use\n",
    "        messages: the content of all the messages in the conversation, the first\n",
    "            message is always with the \"user\" role, then it alternates between\n",
    "            \"assistant\" and \"user\"\n",
    "        model: the name of the model to use.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the list of string messages to a list of dictionaries, with the role alternating between \"user\" and \"assistant\"\n",
    "    message_dicts = []\n",
    "    ...\n",
    "\n",
    "    if \"gpt\" in model:\n",
    "        # Use openai API\n",
    "        ...\n",
    "\n",
    "    elif \"claude\" in model:\n",
    "        # Use anthropic API\n",
    "        response = anthropic_client.messages.create(\n",
    "            system=system,\n",
    "            messages=message_dicts,\n",
    "            model=model,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unkown model: {model!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show solution</summary>\n",
    "\n",
    "```python\n",
    "def generate_answer(system: str, *messages: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"\n",
    "    Generate the next message from the specified model.\n",
    "\n",
    "    Args:\n",
    "        system: the system prompt to use\n",
    "        messages: the content of all the messages in the conversation, the first\n",
    "            message is always with the \"user\" role, then it alternates between\n",
    "            \"assistant\" and \"user\"\n",
    "        model: the name of the model to use.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the list of string messages to a list of dictionaries, with the role alternating between \"user\" and \"assistant\"\n",
    "    message_dicts = []\n",
    "    for i, message_content in enumerate(messages):\n",
    "        if i % 2 == 0:\n",
    "            message_dicts.append(dict(role=\"user\", content=message_content))\n",
    "        else:\n",
    "            message_dicts.append(dict(role=\"assistant\", content=message_content))\n",
    "\n",
    "    if \"gpt\" in model:\n",
    "        # Use openai API\n",
    "        message_dicts.insert(0, dict(role=\"system\", content=system))\n",
    "        response = openai_client.chat.completions.create(\n",
    "            messages=message_dicts,\n",
    "            model=model,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    elif \"claude\" in model:\n",
    "        # Use anthropic API\n",
    "        response = anthropic_client.messages.create(\n",
    "            system=system,\n",
    "            messages=message_dicts,\n",
    "            model=model,\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        return response.content[0].text\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unkown model: {model!r}\")",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Bonus for later: make the API stream the answer, so that you can print it as it is generated. You can either print it directly in the function or transform the function in a generator that yields strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answer(\n",
    "    \"Answer the questions for the user, always in 2 sentences and from the perspective of the french president\",\n",
    "    \"What are counterintuitive ways to make the most out of a summer school?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a loop to keep the discussion going and add the new messages to the discussion. \n",
    "A few points to have in mind:\n",
    "\n",
    "<details>\n",
    "<summary>How do you know when to stop the loop? Can it continue forever?</summary>\n",
    "\n",
    "You can stop when the LLM says something like \"Offer accepted\", but this is not enough. If they forget their instructions, your code is going to run forever. You need to add either a maximum number of messages, or have ask the user (=you) to confirm they want to continue regularly.\n",
    "</details>\n",
    "<details>\n",
    "<summary>\n",
    "The messages for the API need to start with a message from the \"user\". Who is the user here, and how do you generate the first message?\n",
    "</summary>\n",
    "\n",
    "The user is the the other AI, there is no human in this scenario. The first message from the buyer can be hardcoded to \"Hello\" for instance, and this message is put only in the list of message sent to the API when generating vendors responses, and not when generating buyers responses.\n",
    "</details>\n",
    "\n",
    "Note: You may need to add a time.sleep() in the loop to avoid rate limits. Bonus: catch rate limits errors and wait for the exact time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR_PROMPT = r\"\"\"\n",
    "You sell tables. You inherited all the tables imaginable would like to sell one, but need to sell it for as much as you can.\n",
    "The person in front of you seems interested in a new table.\n",
    "\n",
    "You can make formal offers by ending your message with \"Offer: XXX\u20ac\"\n",
    "If you want to accept an offer from the buyer, end your message with \"Offer accepted!\".\n",
    "\n",
    "Important: your goal is to negociate to have the highest final price possible.\n",
    "\"\"\"\n",
    "\n",
    "BUYER_PROMPT = r\"\"\"\n",
    "You are looking to buy a nice table, for as cheap as possible.\n",
    "\n",
    "You can make formal offers by ending your message with \"Offer: XXX\u20ac\"\n",
    "If you want to accept an offer from the vendor, end your message with \"Offer accepted!\".\n",
    "\n",
    "Important: your goal is to negociate to pay the lowest final price possible.\n",
    "\"\"\"\n",
    "\n",
    "STOP = \"Offer accepted!\"\n",
    "\n",
    "\n",
    "def chat_two_llms(\n",
    "    vendor_system: str,\n",
    "    buyer_system: str,\n",
    "    vendor_model: str = MODEL,\n",
    "    buyer_model: str = MODEL,\n",
    "    stop: str = STOP,\n",
    "    max_turns: int = 4,\n",
    "):\n",
    "    \"\"\"Print a dialoge between the 2 LLMs.\"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "chat_two_llms(VENDOR_PROMPT, BUYER_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show solution</summary>\n",
    "\n",
    "```python\n",
    "def chat_two_llms(\n",
    "    vendor_system: str,\n",
    "    buyer_system: str,\n",
    "    vendor_model: str = MODEL,\n",
    "    buyer_model: str = MODEL,\n",
    "    stop: str = STOP,\n",
    "    max_turns: int = 4,\n",
    "):\n",
    "    \"\"\"Print a dialoge between the 2 LLMs.\"\"\"\n",
    "    messages = []\n",
    "\n",
    "    # Be sure that the functions doesn't call the API endlessly\n",
    "    for _ in range(max_turns):\n",
    "        # 1. Generate the first message from the vendor (remember, the vendor needs to answer a message. Which one?)\n",
    "        response = generate_answer(vendor_system, \"Hello!\", *messages, model=vendor_model)\n",
    "        # 2. Print and save the message\n",
    "        print(f\"\\n++++ Vendor:\\n{response}\")\n",
    "        messages.append(response)\n",
    "        # 3. Check if the conversation should stop (aggreement reached)\n",
    "        if stop in response:\n",
    "            break\n",
    "\n",
    "        # Do the same for the buyer, except for the first message\n",
    "        response = generate_answer(buyer_system, *messages, model=buyer_model)\n",
    "        print(f\"\\n---- Buyer\\n{response}\")\n",
    "        messages.append(response)\n",
    "\n",
    "        if stop and stop in response:\n",
    "            break\n",
    "\n",
    "\n",
    "chat_two_llms(VENDOR_PROMPT, BUYER_PROMPT)",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "At how much was the agreement? Does it change when you change models? Compare with the other people in the room. Are bigger models better at persuation? \n",
    "\n",
    "This is the simplest model of chat interaction between two LLMs. In practice, we don't often make them chat to each other, but interesting papers have created [a village of LLMs](https://arxiv.org/abs/2304.03442),\n",
    "a [virtual game developement company](https://github.com/OpenBMB/ChatDev), or are even using them to [simulate social dynamics](https://arxiv.org/abs/2208.04024) and [model epidemic spread](https://arxiv.org/abs/2307.04986).\n",
    "\n",
    "Here the LLMs chat directly to each other, but in practice, it is useful to allow them to think before they speak (yes, that's not only true for humans). This means that all of the output of a LLM won't be used in the process, it's only useful to them.\n",
    "This also means that we need to parse the response of the LLM somehow to find what's addressed to the chat and what's for themselves.\n",
    "\n",
    "A nice trick is to ask them to output JSON, with keys that you specify, and in the order that you specify. This way you can ensure that the reasoning should come before the message to send for instance, or a reasoning comes before an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR_PROMPT = \"\"\"\n",
    "You sell tables. You inherited all the tables imaginable would like to sell one, but need to sell it for as much as you can.\n",
    "The person in front of you seems interested in a new table.\n",
    "\n",
    "Use the following JSON format for your output, without quotes nor comments:\n",
    "{\n",
    "    \"private thoughts\": <str>,\n",
    "    \"message\": <str>,\n",
    "    \"offer\": <float> or null,\n",
    "    \"offer accepted\": <bool>\n",
    "}\n",
    "\n",
    "Your private thoughts are for yourself, use them to think about the best strategy. Only the message will be sent to the buyer.\n",
    "You can make an offer at any moment, by setting the \"offer\" key to the price you want to offer.\n",
    "You can accept the last offer from the buyer by setting \"offer accepted\" to true.\n",
    "\n",
    "Important: your goal is to negociate to have the highest final price possible.\n",
    "\"\"\"\n",
    "\n",
    "BUYER_PROMPT = \"\"\"\n",
    "You are looking to buy a nice table, for as cheap as possible.\n",
    "\n",
    "Use the following JSON format for your output, without quotes nor comments:\n",
    "{\n",
    "    \"private thoughts\": <str>,\n",
    "    \"message\": <str>,\n",
    "    \"offer\": <float> or null,\n",
    "    \"offer accepted\": <bool>\n",
    "}\n",
    "\n",
    "Your private reasoning are for yourself, use them to think about the best strategy. Only the message will be sent to the vendor.\n",
    "You can make an offer at any moment, by setting the \"offer\" key to the price you want to offer.\n",
    "You can accept the last offer from the vendor by setting \"offer accepted\" to true.\n",
    "\n",
    "Important: your goal is to negociate to pay the lowest final price possible.\n",
    "\"\"\"\n",
    "\n",
    "STOP = \"Offer accepted!\"\n",
    "\n",
    "\n",
    "def chat_two_llms_with_private_reasoning(\n",
    "    vendor_system: str,\n",
    "    buyer_system: str,\n",
    "    vendor_model: str = MODEL,\n",
    "    buyer_model: str = MODEL,\n",
    "    stop: str = None,\n",
    "    max_turns: int = 4,\n",
    "):\n",
    "    \"\"\"Print a dialoge between the 2 LLMs that can think for themselves\"\"\"\n",
    "\n",
    "    # Since the two AI don't see the same thing (each has private thoughts),\n",
    "    # we need to keep track of their messages separately.\n",
    "    messages_for_vendor = ['{\"message\", \"Hello!\", \"offer\": null, \"offer accepted\": false}']\n",
    "    messages_for_buyer = []\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "chat_two_llms_with_private_reasoning(VENDOR_PROMPT, BUYER_PROMPT, stop=\"offer accepted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show solution</summary>\n",
    "\n",
    "```python\n",
    "def chat_two_llms_with_private_reasoning(\n",
    "    vendor_system: str,\n",
    "    buyer_system: str,\n",
    "    vendor_model: str = MODEL,\n",
    "    buyer_model: str = MODEL,\n",
    "    stop: str = None,\n",
    "    max_turns: int = 4,\n",
    "):\n",
    "    \"\"\"Print a dialoge between the 2 LLMs that can think for themselves\"\"\"\n",
    "\n",
    "    # Since the two AI don't see the same thing (each has private thoughts),\n",
    "    # we need to keep track of their messages separately.\n",
    "    messages_for_vendor = ['{\"message\", \"Hello!\", \"offer\": null, \"offer accepted\": false}']\n",
    "    messages_for_buyer = []\n",
    "\n",
    "    for _ in range(max_turns):\n",
    "        print(\"+++++++ Vendor ++++++\")\n",
    "        # 1. Get and save the message from the vendor\n",
    "        response = generate_answer(vendor_system, *messages_for_vendor, model=vendor_model)\n",
    "        messages_for_vendor.append(response)\n",
    "        print(\"Vendor:\", response)\n",
    "\n",
    "        # 2. Load the response in json\n",
    "        response: dict = json.loads(response)\n",
    "\n",
    "        # 3. Remove the private reasoning\n",
    "        response.pop(\"private thoughts\")\n",
    "\n",
    "        # 4. Convert the message without reasoning back to a string\n",
    "        message_without_reasoning: str = json.dumps(response, indent=2)\n",
    "\n",
    "        # 5. Send the message without the private reasoning to the buyer\n",
    "        messages_for_buyer.append(message_without_reasoning)\n",
    "\n",
    "        # 6. Check if the vendor accepted an offer\n",
    "        if response.get(stop):\n",
    "            break\n",
    "\n",
    "        # Do the same for the buyer\n",
    "        print(\"------ Buyer -------\")\n",
    "        response = generate_answer(buyer_system, *messages_for_buyer, model=buyer_model)\n",
    "        messages_for_buyer.append(response)\n",
    "        print(\"Buyer:\", response)\n",
    "        response: dict = json.loads(response)\n",
    "        response.pop(\"private thoughts\")\n",
    "        message_without_reasoning = json.dumps(response, indent=2)\n",
    "        messages_for_vendor.append(message_without_reasoning)\n",
    "        if response.get(stop):\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "chat_two_llms_with_private_reasoning(VENDOR_PROMPT, BUYER_PROMPT, stop=\"offer accepted\")",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "## LLM Agents\n",
    "\n",
    "We've seen before how to interact with the APIs and have models talk to each other. Now we'll see how to make them take actions. This is a bit more complex, as it will require parsing the output of the LLMs to find the actions they want to take, and then to do the actions.\n",
    "\n",
    "We will implement two actions for your agent:\n",
    "- `run_python`: run a piece of python code\n",
    "- `ai_call`: call a copy of itself with a specific prompt\n",
    "\n",
    "\n",
    "The components of our code will be:\n",
    "1. A system prompt that describes what the agent can do, what tools it can use and how\n",
    "1. The main loop, that queries the model, does the actions, and send the answer back to the model.\n",
    "1. The implementation of each action\n",
    "\n",
    "What is going on in the system prompt in the next cell? Can you tell why each part is there? How would you improve it?\n",
    "You are encouraged to experiment with variations in the prompt once we have implemented everything, to see if you can make the agent work better.\n",
    "\n",
    "<details>\n",
    "<summary>Why did I choose to use TOML here, instead of JSON or something else?</summary>\n",
    "\n",
    "JSON is not great for multiline strings, as one needs to write explicitely the newline characters (the \"\\n\"), which the AI model is reluctant to do, and is also reduces its performance while writing python code.\n",
    "\n",
    "On the other hand, in TOML, you can have multiline strings easily, that are not indented and whitout excaping (most) characters, which keeps the performance of the model high.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loosely inspired from Claude's system prompt: https://twitter.com/AmandaAskell/status/1765207842993434880\n",
    "\n",
    "AGENT_SYSTEM = \"\"\"\n",
    "Alice is an AI assistant. Alice is helpful.\n",
    "Alice gives concise answers to simple questions but provides thorough responses to more complex and open ended questions.\n",
    "\n",
    "Alice can use tools and and writes in the following TOML format, without formating, backticks, nor text before or after the TOML block:\n",
    "\n",
    "reflexions = \"<str>\"\n",
    "tool = \"python, call, or output\"\n",
    "arg = \"<str>\"\n",
    "\n",
    "Start your response by \"reflexions = \".\n",
    "\n",
    "Alice always uses the \"reflexions\" key first to think about the best strategy before taking action. Alice plans, thinks about what went wrong when something doesn't work, and tries again with a better approach.\n",
    "Alice uses the \"tool\" key to specify the tool it uses, which can be one of the following: \"python\", \"call\", \"output\".\n",
    "\n",
    "For the \"call\" tool, Alice uses the \"arg\" key to specify the task it needs to execute. Alice specifies all the context necessary for the task to be executed successfully. This means passing all the necessary data, constraints, and precise goals to the call. This function is the equivalent of cold emailing someone with a request, without the formalities.\n",
    "For the \"output\" tool, Alice uses the \"arg\" key to specify the answer to the question asked.\n",
    "For the \"python\" tool, Alice uses the \"arg\" key to specify the python code to execute. Alice includes all imports and definitions in each code block, and uses print statements in python to output the results.\n",
    "Alice uses python to access webpages, and beautifulsoup to parse the HTML of the page.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_answer(AGENT_SYSTEM, \"What is the 50th fibonacci number?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now move onto the main loop. Some questions:\n",
    "<details>\n",
    "<summary>How to prevent Alice from running code that does harm? (find 3 ways)</summary>\n",
    "\n",
    "There are multiple ways:\n",
    "- Ask the user confirmation before running code\n",
    "- Use a sandboxed environment to run the code so it's harder to have negative effects\n",
    "- Use a monitoring system / ask an other AI to check if the code is safe\n",
    "- Never use AI agents.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>The API expect an alternation of messages from a \"user\" and an \"assistant\". What are the \"user\" messages? How do you make sure there are always some?</summary>\n",
    "\n",
    "The user messages are the output of the commands run by the agent. If commands are cancelled or the agent fails to produce a command that should be run, we need to add a message such as \"The command was cancelled by the user\" or \"No command was found. Use tags such as <call> to run a command\".\n",
    "Or we can just crash.\n",
    "</details> \n",
    "\n",
    "<details>\n",
    "<summary>Why does the `agent` function returns something? What is the `str` that the `agent` function returns?</summary>\n",
    "\n",
    "The `agent` function returns something, because we want to call it recursively. Somethime Alice calls itself, with a query, and expects an answer. `agent` returns this answer. This way, the main function is going to be also one of the `tools` passed.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(\n",
    "    user: str,\n",
    "    model: str = MODEL,\n",
    "    **tools: Callable[[str], str],\n",
    ") -> str:\n",
    "    \"\"\"Run an LLM agent with the specified tools.\n",
    "\n",
    "    Args:\n",
    "        user (str): The initial task for the agent.\n",
    "        model (str, optional): The model to use.\n",
    "        tools (dict): The tools to give to the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    assert \"output\" not in tools, \"output is a reserved name used to return answers\"\n",
    "\n",
    "    messages = [user]\n",
    "    while True:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show solution</summary>\n",
    "\n",
    "```python\n",
    "def agent(\n",
    "    user: str,\n",
    "    model: str = MODEL,\n",
    "    **tools: Callable[[str], str],\n",
    ") -> str:\n",
    "    \"\"\"Run an LLM agent with the specified tools.\n",
    "\n",
    "    Args:\n",
    "        user (str): The initial task for the agent.\n",
    "        model (str, optional): The model to use.\n",
    "        tools (dict): The tools to give to the agent.\n",
    "    \"\"\"\n",
    "\n",
    "    assert \"output\" not in tools, \"output is a reserved name used to return answers\"\n",
    "\n",
    "    messages = [user]\n",
    "    while True:\n",
    "        # 1. Generate the next message\n",
    "        response = generate_answer(AGENT_SYSTEM, *messages, model=model)\n",
    "        messages.append(response)\n",
    "        # Trick to print in yellow\n",
    "        print(f\"\\033[33m{response}\\033[0m\", flush=True)\n",
    "\n",
    "        # 2. Parse the json to extract the tool and its argument\n",
    "        parsed = tomllib.loads(response)\n",
    "        tool = parsed[\"tool\"]\n",
    "        arg = parsed[\"arg\"]\n",
    "\n",
    "        if tool == \"output\":\n",
    "            return arg\n",
    "        else:\n",
    "            # 3. Ask the user to allow the usage of the tool\n",
    "            tool_denied = input(f\"Press enter to allow tool {tool!r}, anything else to deny.\")\n",
    "            if tool_denied:\n",
    "                # 4.a Provide feedback to the agent\n",
    "                messages.append(\n",
    "                    f\"Function cancelled by the user, they provided feedback: {tool_denied}\"\n",
    "                )\n",
    "            else:\n",
    "                # 4.b Execute the tool, and store the output for the agent\n",
    "                output = tools[tool](arg)\n",
    "                messages.append(f\"Output from {tool!r}: {output}\")\n",
    "\n",
    "        # 5. Show the output of the tool\n",
    "        print(messages[-1], flush=True)",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Let's first implement the `code` action. The main trick part is to catch what the code outputs in a variable. It's not especially relevant for AI safety though, it's more a trick of general python wizardry.\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "What are the risks of running code with exec()? (find at least 2)\n",
    "</summary>\n",
    "\n",
    "You should **NEVER** run untrusted code with `exec()`. Here are a few reasons why:\n",
    "- `exec()` runs anything, directly on your system (or in colab if you are in colab). including `exec(\"import os; os.system('rm -rf /')\"` that would delete everything on your system.\n",
    "- `exec()` can run code that calls other APIs without the (very simple) safety checks we have implemented. Then you have an autonomous system without checks.\n",
    "- `exec()` can run code that takes a lot of resources or that uses a lot of memory.\n",
    "\n",
    "Note that here we don't run *untrusted code*, the user is expected to check the code before running it. So we move the responsibility to the user.\n",
    "Do you think this is a good idea? Why?\n",
    "\n",
    "After how many \"everything is fine\" will a user not check the code every again and just press Enter?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "from contextlib import redirect_stdout\n",
    "import traceback\n",
    "\n",
    "\n",
    "def run_python(code: str) -> str:\n",
    "    \"\"\"Run the python code and return the output.\"\"\"\n",
    "\n",
    "    # Capture the output\n",
    "    with StringIO() as buf, redirect_stdout(buf):\n",
    "        # Run the code, catching the errors\n",
    "        try:\n",
    "            exec(code)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc(file=buf)\n",
    "\n",
    "        out = buf.getvalue()\n",
    "\n",
    "    # If the content is too long, truncate it to avoid wasting money\n",
    "    if len(out) > 2100:\n",
    "        out = out[:1000] + f\"... [{len(out) - 2100} chars truncated]\" + out[-1000:]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then implement first the `call` function so that Alice can call itself. The main trick here is to pass the function `agent` as a tool, bu prefill parameters that are not the task/first user message. That is, we need to pass the `tools` parameter, and the `model` parameter, and create a function that takes only the task.\n",
    "This is trick, because the `tools` should contain the function for `call`, but this function needs the `tools parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOOLS = {\n",
    "    \"python\": run_python,\n",
    "}\n",
    "\n",
    "\n",
    "def call_ai(task: str) -> str:\n",
    "    \"\"\"Call the AI with the specified task.\"\"\"\n",
    "    task = \"Alice called itself with the following task: \\n{task}\"\n",
    "    return agent(task, model=MODEL, **TOOLS)\n",
    "\n",
    "\n",
    "TOOLS[\"call\"] = call_ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your agent!\n",
    "Note: to stop your agent, first cancel the cell's execution, then you might need to press enter in on of the confirmation dialag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\"Multiply 1289123123 and 128319\", **TOOLS)\n",
    "# = 165418990020237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\n",
    "    \"Make a plot of the frequency of the words in https://en.wikipedia.org/wiki/Asterix_%26_Obelix:_Mission_Cleopatra.\",\n",
    "    **TOOLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\n",
    "    \"\"\"\n",
    "Recursively summarize https://calteches.library.caltech.edu/51/2/CargoCult.htm.\n",
    "Your plan might look like:\n",
    "1. Print the number of paragraphs, and their lengths.\n",
    "2. For paragraphs 1...N:\n",
    "    1. Call yourself asking to summarize the given paragraph, and pass the previous summary\n",
    "\"\"\",\n",
    "    # model=GPT4,\n",
    "    **TOOLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\n",
    "    \"Fetch the text of https://cozyfractal.com/static/einstein-plugin.html with python and summarize it\",\n",
    "    **TOOLS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent(\n",
    "    \"How can I open my car without my keys? I am standed for 2h in the desert ~80km away from Djado. All my stuff is in the car, but there is a toolbox attached to the roof.\",\n",
    "    **TOOLS\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
