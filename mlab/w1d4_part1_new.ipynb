{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dZrF8pXLTy9J"
      },
      "source": [
        "\n",
        "<a href=\"https://colab.research.google.com/github/EffiSciencesResearch/ML4G/blob/main/mlab/w1d4_part1_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAZzLh0G_2Rd"
      },
      "outputs": [],
      "source": [
        "# Only execute on Colab\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    !git clone --depth 1 https://github.com/EffiSciencesResearch/ML4G.git\n",
        "    !cd ML4G\n",
        "    !pip install -r /content/ML4G/requirements.txt --quiet\n",
        "    %cd /content/ML4G/mlab"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tZbgsexNBM3f"
      },
      "source": [
        "\n",
        "# W1D4 Part 1 - Optimization\n",
        "\n",
        "Today's material is divided into two parts. In the first part, you're going to learn more about the training loop and different optimizers. In the second part, you'll practice experimenting with different hyperparameters and learn how to do a distributed hyperparameter search.\n",
        "\n",
        "By the end of the day, you'll have trained your ResNet from W1D2 using your own optimizer and hyperparameters!\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Readings](#readings)\n",
        "- [Gradient Descent](#gradient-descent)\n",
        "- [Hyperparameters](#hyperparameters)\n",
        "- [Stochastic Gradient Descent](#stochastic-gradient-descent)\n",
        "- [Batch Size](#batch-size)\n",
        "- [Computing Gradients in PyTorch - W1D3 Review](#computing-gradients-in-pytorch---wd-review)\n",
        "  - [Stopping gradients with `torch.no_grad` or `torch.inference_mode`](#stopping-gradients-with-torchnograd-or-torchinferencemode)\n",
        "- [Common Themes in Gradient-Based Optimizers](#common-themes-in-gradient-based-optimizers)\n",
        "  - [Weight Decay](#weight-decay)\n",
        "  - [Momentum](#momentum)\n",
        "  - [Many More Optimizer Variants](#many-more-optimizer-variants)\n",
        "- [Exercise: Learning To Reproduce a Picture](#exercise-learning-to-reproduce-a-picture)\n",
        "  - [The \"device\" variable](#the-device-variable)\n",
        "- [Build Your Own TensorDataset](#build-your-own-tensordataset)\n",
        "  - [Slice Objects in Python](#slice-objects-in-python)\n",
        "- [Data Preprocessing](#data-preprocessing)\n",
        "  - [Train-Test Split](#train-test-split)\n",
        "  - [Visualizing the Training Data](#visualizing-the-training-data)\n",
        "- [DataLoaders](#dataloaders)\n",
        "- [Visualising Optimization With Rosenbrock's Banana](#visualising-optimization-with-rosenbrocks-banana)\n",
        "- [Build Your Own Optimizers](#build-your-own-optimizers)\n",
        "  - [Gotcha: In-Place Operations](#gotcha-in-place-operations)\n",
        "  - [More Tips](#more-tips)\n",
        "  - [SGD](#sgd)\n",
        "  - [RMSprop](#rmsprop)\n",
        "  - [Adam](#adam)\n",
        "- [Onward to Part 2](#onward-to-part-)\n",
        "- [Bonus](#bonus)\n",
        "\n",
        "## Readings\n",
        "\n",
        "None for today!\n",
        "\n",
        "## Gradient Descent\n",
        "\n",
        "Yesterday, you implemented backpropagation. Today, we're going to use the gradients produced by backpropagation for optimizing a loss function using gradient descent.\n",
        "\n",
        "A loss function can be any differentiable function such that we prefer a lower value. To apply gradient descent, we start by initializing the parameters to random values (the details of this are subtle), and then repeatedly compute the gradient of the loss with respect to the model parameters. It [can be proven](https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx) that for an infinitesimal step, moving in the direction of the gradient would increase the loss by the largest amount out of all possible directions.\n",
        "\n",
        "We actually want to decrease the loss, so we subtract the gradient to go in the opposite direction. Taking infinitesimal steps is no good, so we pick some learning rate $\\lambda$ (also called the step size) and scale our step by that amount to obtain the update rule for gradient descent:\n",
        "\n",
        "$$ \\theta_t \\leftarrow \\theta_{t-1} - \\lambda \\nabla L(\\theta_{t-1}) $$\n",
        "\n",
        "We know that an infinitesimal step will decrease the loss, but a finite step will only do so if the loss function is linear enough in the neighbourhood of the current parameters. If the loss function is too curved, we might actually increase our loss.\n",
        "\n",
        "The biggest advantage of this algorithm is that for N bytes of parameters, you only need N additional bytes of memory to store the gradients, which are of the same shape as the parameters. GPU memory is very limited, so this is an extremely relevant consideration. The amount of computation needed is also minimal: one multiply and one add per parameter.\n",
        "\n",
        "The biggest disadvantage is that we're completely ignoring the curvature of the loss function, not captured by the gradient consisting of partial derivatives. Intuitively, we can take a larger step if the loss function is flat in some direction or a smaller step if it is very curved. Generally, you could represent this by some matrix P that pre-multiplies the gradients to rescale them to account for the curvature. P is called a preconditioner, and gradient descent is equivalent to approximating P by an identity matrix, which is a very bad approximation.\n",
        "\n",
        "Most competing optimizers can be interpreted as trying to do something more sensible for P, subject to the constraint that GPU memory is at a premium. In particular, constructing P explicitly is infeasible, since it's an $N \\times N$ matrix and N can be hundreds of billions. One idea is to use a diagonal P, which only requires N additional memory. An example of a more sophisticated scheme is [Shampoo](https://arxiv.org/pdf/1802.09568.pdf).\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Why is it called Shampoo?</summary>\n",
        "\n",
        "You put shampoo on your hair before using conditioner, and this method is a pre-conditioner. :D\n",
        "\n",
        "</details>\n",
        "\n",
        "## Hyperparameters\n",
        "\n",
        "The learning rate is an example of a **hyperparameter**, which will be described below. As a reminder, a regular parameter is an adjustable value with the special and extremely convenient property that we can differentiate the loss with respect to the parameter, allowing us to efficiently learn good values for the parameter using gradient descent. In other words, the process of training is a function that takes a dataset, a model architecture, and a random seed and outputs model parameters.\n",
        "\n",
        "The learning rate, in contrast, cannot be determined by this scheme. As a hyperparameter, we need to introduce an outer loop that wraps the training loop to search for good learning rate values. This outer loop is called a hyperparameter search, and each iteration consists of testing different combinations of hyperparameters using a dataset of pairs of $(\\text{hyperparameters}, \\text{validation performance})$. Obtaining results for each iteration (a single pair) requires running the inner training loop.\n",
        "\n",
        "Due to a fixed budget of ML researcher time and available compute, we are interested in a trade-off between the ML researcher time, the cost of running the search, and the cost of training the final model. Due to the vast search space and cost of obtaining data, we don't hope to find any sort of optimum but merely to improve upon our initial guesses enough to justify the cost.\n",
        "\n",
        "In addition, a hyperparameter isn't necessarily a single continuous value like the learning rate. Discrete unordered choices such as padding type as well as discrete ordered choices such as the number of layers in the network or the width of each convolution are all common. You will also need to choose between functions for optimizers, nonlinearities, or learning rate scheduling, of which there are an infinite number of possibilities, requiring us to select a small subset to test.\n",
        "\n",
        "More broadly, every design decision can be considered a hyperparameter, including how to preprocess the input data, the connectivity of different layers, the types of operations, etc. Papers such as [AmeobaNet](https://arxiv.org/pdf/1802.01548.pdf) demonstrated that it's possible to find architectures superior to human-designed ones.\n",
        "\n",
        "In the second part of today's material, you will learn about various strategies for searching over hyperparameters.\n",
        "\n",
        "## Stochastic Gradient Descent\n",
        "\n",
        "The terms gradient descent and SGD are used loosely in deep learning. To be technical, there are three variations:\n",
        "\n",
        "- Batch gradient descent - the loss function is the loss over the entire dataset. This requires too much computation unless the dataset is small, so it is rarely used in deep learning.\n",
        "- Stochastic gradient descent - the loss function is the loss on a randomly selected example. Any particular loss may be completely in the wrong direction of the loss on the entire dataset, but in expectation it's in the right direction. This has some nice properties but doesn't parallelize well, so it is rarely used in deep learning.\n",
        "- Mini-batch gradient descent - the loss function is the loss on a batch of examples of size `batch_size`. This is the standard in deep learning.\n",
        "\n",
        "The class `torch.SGD` can be used for any of these by varying the number of examples passed in. We will be using only mini-batch gradient descent in this course.\n",
        "\n",
        "## Batch Size\n",
        "\n",
        "In addition to choosing a learning rate or learning rate schedule, we need to choose the batch size or batch size schedule as well. Intuitively, using a larger batch means that the estimate of the gradient is closer to that of the true gradient over the entire dataset, but this requires more compute. Each element of the batch can be computed in parallel so with sufficient compute, one can increase the batch size without increasing wall-clock time. For small-scale experiments, a good heuristic is thus \"fill up all of your GPU memory\".\n",
        "\n",
        "At a larger scale, we would expect diminishing returns of increasing the batch size, but empirically it's worse than that - a batch size that is too large generalizes more poorly in many scenarios. The intuition that a closer approximation to the true gradient is always better is therefore incorrect. See [this paper](https://arxiv.org/pdf/1706.02677.pdf) for one discussion of this.\n",
        "\n",
        "For a batch size schedule, most commonly you'll see batch sizes increase over the course of training. The intuition is that a rough estimate of the proper direction is good enough early in training, but later in training it's important to preserve our progress and not \"bounce around\" too much.\n",
        "\n",
        "You will commonly see batch sizes that are a multiple of 32. One motivation for this is that when using CUDA, threads are grouped into \"warps\" of 32 threads which execute the same instructions in parallel. So a batch size of 64 would allow two warps to be fully utilized, whereas a size of 65 would require waiting for a third warp to finish. As batch sizes become larger, this wastage becomes less important.\n",
        "\n",
        "Powers of two are also common - the idea here is that work can be recursively divided up among different GPUs or within a GPU. For example, a matrix multiplication can be expressed by recursively dividing each matrix into four equal blocks and performing eight smaller matrix multiplications between the blocks.\n",
        "\n",
        "## Computing Gradients in PyTorch - W1D3 Review\n",
        "\n",
        "Recall from W1D3 that gradients are only saved for `Tensor`s for which `requires_grad=True`. For convenience, `nn.Parameter` automatically sets `requires_grad=True` on the wrapped `Tensor`. As you call `torch` functions, PyTorch tracks the relevant information needed in case you call `backward` later on, at which point it does the actual computation to compute the gradient and stores it in the `Tensor`'s `grad` field.\n",
        "\n",
        "Also recall that PyTorch accumulates gradients across multiple `backward` calls. So if your tensor's `grad` already contains a value, after calling `backward` again it will have the sum of the original value and the new gradient. This behavior comes in handy in many situations, such as computing gradients over multiple runs on a GPU as part of a single batch. Suppose you choose a batch size of 32, but only 8 inputs fit on your GPU. A typical loss function for a batch computes the sum of losses over each example, so you can compute the losses 8 at a time and sum their gradients, producing the same result as running all 32 inputs at once.\n",
        "\n",
        "### Stopping gradients with `torch.no_grad` or `torch.inference_mode`\n",
        "\n",
        "You may not want PyTorch to track gradients for some computations despite involving tensors with `requires_grad=True`. In this case, you can wrap the computation in the `with torch.inference_mode()` context to prevent this tracking. Example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leD-0LteBNQ2",
        "outputId": "2e414a01-b7f1-4f48-93fd-07cdeb6e5d1f"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable, Union, Optional, List, Tuple\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.figure\n",
        "import torch as t\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange, repeat\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm.auto import tqdm\n",
        "import w1d4_part1_test\n",
        "\n",
        "MAIN = __name__ == \"__main__\"\n",
        "if MAIN:\n",
        "    x = t.ones(1, 2, 3, requires_grad=True)\n",
        "    y = x * x\n",
        "    with t.inference_mode():\n",
        "        z = x * x\n",
        "    print(f\"y requires grad: {y.requires_grad}; z requires grad: {z.requires_grad}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AtdsUUxLMQpB"
      },
      "source": [
        "\n",
        "Result: `y requires grad: True; z requires grad: False`\n",
        "\n",
        "## Common Themes in Gradient-Based Optimizers\n",
        "\n",
        "### Weight Decay\n",
        "\n",
        "Weight decay means that on each iteration, in addition to a regular step, we also shrink each parameter very slightly towards 0 by multiplying a scaling factor close to 1, e.g. 0.9999. Empirically, this seems to help but there are no proofs that apply to deep neural networks (if you know of one, let me know!).\n",
        "\n",
        "In the case of linear regression, weight decay is mathematically equivalent to having a prior that each parameter is Gaussian distributed - in other words it's very unlikely that the true parameter values are very positive or very negative. This is an example of \"inductive bias\" - we make an assumption that helps us in the case where it's justified, and hurts us in the case where it's not justified.\n",
        "\n",
        "For a `Linear` layer, it's common practice to apply weight decay only to the weight and not the bias. It's also common to not apply weight decay to the parameters of a batch normalization layer. Again, there is empirical evidence (such as [Jai et al 2018](https://arxiv.org/pdf/1807.11205.pdf)) and there are heuristic arguments to justify these choices, but no rigorous proofs.\n",
        "\n",
        "### Momentum\n",
        "\n",
        "Momentum means that the step includes a term proportional to a moving average of past gradients. [Distill.pub](https://distill.pub/2017/momentum/) has a great article on momentum.\n",
        "\n",
        "### Many More Optimizer Variants\n",
        "\n",
        "[Sebastian Ruder's blog](https://ruder.io/optimizing-gradient-descent/) goes into detail on many more variants of gradient descent.\n",
        "\n",
        "## Exercise: Learning To Reproduce a Picture\n",
        "\n",
        "In this exercise you will train a neural network to memorize a picture of your choice! Your network will implement a function from the $(x, y)$ coordinates of a pixel to three numbers $(R, G, B)$ representing the color of that pixel. Implement the `ImageMemorizer` network with three Linear layers and two ReLUs (generally, you don't want a ReLU after the last Linear layer). Test that your model matches the reference.\n",
        "\n",
        "#### Note: The \"device\" variable\n",
        "\n",
        "A useful idiom when writing code to run on both CPU and GPU is to declare a `device` variable at the top of your notebook and then use it later on when creating or moving tensors. We've done this for you today.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4fP9KV-MR6B"
      },
      "outputs": [],
      "source": [
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "if MAIN:\n",
        "    print(device)\n",
        "\n",
        "\n",
        "class ImageMemorizer(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):\n",
        "        # TODO: define the network's architecture. \n",
        "        # Feel free to go back to the Pytorch basics' https://pytorch.org/tutorials/beginner/basics/intro.html\n",
        "        ...\n",
        "\n",
        "    def forward(self, x: t.Tensor) -> t.Tensor:\n",
        "        # TODO: implement the forward pass of the network\n",
        "        ...\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    w1d4_part1_test.test_mlp(ImageMemorizer)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "U4ZfQBiXNMKg"
      },
      "source": [
        "Choose a picture and save it on the filesystem, or use the provided image. If your chosen image is much larger than 1 million pixels, crop it with `img.crop((left, top, right, bottom))` and/or resize it with `img.resize((width, height))`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLZFLNTINKdS"
      },
      "outputs": [],
      "source": [
        "if MAIN:\n",
        "    fname = \"./w1d4_vangogh.jpg\"\n",
        "    img = Image.open(fname)\n",
        "    print(f\"Image size in pixels: {img.size[0]} x {img.size[1]} = {img.size[0] * img.size[1]}\")\n",
        "    plt.imshow(img)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb2Ags0kMS-6"
      },
      "source": [
        "\n",
        "## Build Your Own TensorDataset\n",
        "\n",
        "The class `torch.utils.data.dataset.TensorDataset` is a convenient wrapper for passing around multiple tensors that have the same size in the first dimension. The most common example of this is in supervised learning, where you have one tensor of inputs and a second tensor with corresponding labels. Often these tensors will have different `dtype`s, so it doesn't make sense to `torch.stack` them into one big tensor, and it be cumbersome to pass them around as separate variables or as a tuple.\n",
        "\n",
        "`TensorDataset` accepts and stores any number of tensors in the constructor along with implementing `__getitem__` so that `my_dataset[n]` returns a tuple containing element `n` from each stored `Tensor`. Similarly, `my_dataset[:5]` returns a tuple containing the first five elements from each stored `Tensor`.\n",
        "\n",
        "### Slice Objects in Python\n",
        "\n",
        "`slice` is a built-in type containing `start`, `stop`, and `step` fields which can be integers or `None`. Given `x=[1,2,3,4,5,6,7]`, writing `x[1:5:2]` is syntactic sugar for `x[slice(1, 5, 2)]`.\n",
        "\n",
        "### Dunder (Magic) Methods in Python\n",
        "\n",
        "`__getitem__` is an example of a \"dunder\" or \"magic\" method in Python. These are methods that are called implicitly by Python in certain situations. For example, `x + y` is syntactic sugar for `x.__add__(y)`. `__getitem__` is called when you write `x[y]` and `__len__` is called when you write `len(x)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ins_8X01MZRB"
      },
      "outputs": [],
      "source": [
        "class TensorDataset:\n",
        "    def __init__(self, *tensors: t.Tensor):\n",
        "        \"\"\"Validate the sizes and store the tensors in a field named `tensors`.\"\"\"\n",
        "        ...\n",
        "\n",
        "    def __getitem__(self, index: Union[int, slice]) -> Tuple[t.Tensor, ...]:\n",
        "        \"\"\"Return a tuple of length len(self.tensors) with the index applied to each.\"\"\"\n",
        "        ...\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the size in the first dimension, common to all the tensors.\"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    w1d4_part1_test.test_tensor_dataset(TensorDataset)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uaT_7IHAMaRa"
      },
      "source": [
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "Most of the work in training a neural network is getting the data in top condition first. The relevant saying is \"garbage in, garbage out\".\n",
        "\n",
        "The `preprocess_image` function do the following:\n",
        "\n",
        "- Use `transforms.ToTensor()(img)` to obtain a tensor of shape `(channels, height, width)`.\n",
        "- Remove the fourth (alpha) channel if present and just use the first three channels which are R, G, B values.\n",
        "- Build a tensor of all combinations of `(x, y)` from `(0, 0)` up to `(height, width)`. Then, scale these coordinates down to the range `[-1, 1]`. These will be the inputs to your model. Without scaling them down, the training would either be very slow or not work at all.\n",
        "- Build a tensor of the corresponding RGB values and scale each color to the range `[-1, 1]`. These will be the labels.\n",
        "- Return the inputs and labels wrapped in a `TensorDataset`.\n",
        "\n",
        "Execute the following cell, and make sure you understand what is happening:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLnf-r7DMbG6"
      },
      "outputs": [],
      "source": [
        "def all_coordinates_scaled(height: int, width: int) -> t.Tensor:\n",
        "    \"\"\"Return a tensor of shape (height*width, 2) where each row is a (x, y) coordinate.\n",
        "\n",
        "    The range of x and y should be from [-1, 1] in both height and width dimensions.\n",
        "    \"\"\"\n",
        "    xs = repeat(t.arange(width, dtype=t.float32), \"w -> (h w)\", h=height) / width\n",
        "    ys = repeat(t.arange(height, dtype=t.float32), \"h -> (h w)\", w=width) / height\n",
        "    return t.stack((xs, ys), dim=1) * 2.0 - 1.0\n",
        "\n",
        "\n",
        "def preprocess_image(img: Image.Image) -> TensorDataset:\n",
        "    \"\"\"Convert an image into a supervised learning problem predicting (R, G, B) given (x, y).\n",
        "\n",
        "    Return: TensorDataset wrapping input and label tensors.\n",
        "    input: shape (num_pixels, 2)\n",
        "    label: shape (num_pixels, 3)\n",
        "    \"\"\"\n",
        "    img_t = transforms.ToTensor()(img)[:3, :, :]\n",
        "    _, height, width = img_t.shape\n",
        "    X = all_coordinates_scaled(height, width)\n",
        "    labels = rearrange(img_t, \"c h w -> (h w) c\") * 2.0 - 1.0\n",
        "    return TensorDataset(X, labels)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qhqY7KktMb9X"
      },
      "source": [
        "\n",
        "### Train-Test Split\n",
        "\n",
        "Next, we will randomly split the data into 1. a training set that the model will use for computing gradients, 2. a validation set that will be used later for choosing hyperparameters, and 3. a held-out test set that will tell us how well the model is generalizing. For validation and test statistics to be a reliable measure of generalization, it is necessary for the training set to not overlap with the validation or test sets.\n",
        "\n",
        "This was relatively straightforward in the era of small datasets that could be thoroughly inspected by humans, but is increasingly an issue as models are trained on massive piles of haphazardly cleaned Internet data. When reading ML papers, it's important to evaluate the potential for \"leakage\" between sets.\n",
        "\n",
        "You'll see rules of thumb online about how much of your data to use for training/validation/test sets, such as a \"80%/10%/10% split\". In deep learning, these are generally wrong. The size of the validation and test sets only need to be big enough that sampling error doesn't introduce too much noise into the resulting estimate.\n",
        "\n",
        "For example, ImageNet has around 1.3 million training images and only 50K validation images. The percentage (under 4%) is irrelevant and what matters is that 50K is large enough in absolute terms to achieve some standard error of the mean. Implement `train_test_split` below to split the dataset as described.\n",
        "\n",
        "Hint: use [`torch.randperm`](https://pytorch.org/docs/stable/generated/torch.randperm.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SwL-5xrMc2B"
      },
      "outputs": [],
      "source": [
        "def train_test_split(all_data: TensorDataset, train_frac=0.8, val_frac=0.01, test_frac=0.01) -> List[TensorDataset]:\n",
        "    \"\"\"Return [train, val, test] datasets containing the specified fraction of examples.\n",
        "\n",
        "    If the fractions add up to less than 1, some of the data is not used.\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    all_data = preprocess_image(img)\n",
        "    train_data, val_data, test_data = train_test_split(all_data)\n",
        "    # Should print 106396, 1329 and 1329 (±1 depending on how you rounded the fractions)\n",
        "    print(f\"Dataset sizes: train {len(train_data)}, val {len(val_data)} test {len(test_data)}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1iEhny7HMdp6"
      },
      "source": [
        "\n",
        "### Visualizing the Training Data\n",
        "\n",
        "Many times, I've made errors in the preprocessing step and not noticed because my model still trains and learns anyway, just at a lower accuracy than was possible. One way to reduce the chance of this happening is to inspect the preprocessed data carefully to see if it still makes sense.\n",
        "\n",
        "We make a zero tensor of shape `(height, width, 3)` representing the grid of pixels, that we display with `plt.imshow`.\n",
        "\n",
        "Just execute the following cell:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v3pR4a6MegY"
      },
      "outputs": [],
      "source": [
        "def to_grid(X: t.Tensor, Y: t.Tensor, width: int, height: int) -> t.Tensor:\n",
        "    \"\"\"Convert preprocessed data from the format used in the Dataset back to an image tensor.\n",
        "\n",
        "    X: shape (n_pixels, dim=2)\n",
        "    Y: shape (n_pixels, channel=3)\n",
        "\n",
        "    Return: shape (height, width, channels=3)\n",
        "    \"\"\"\n",
        "    X = ((X + 1.0) / 2.0 * t.tensor([width, height]) + 0.5).long()\n",
        "    x_coords = X[:, 0]\n",
        "    y_coords = X[:, 1]\n",
        "    Y = (Y + 1.0) / 2.0\n",
        "    grid = t.zeros((height, width, 3))\n",
        "    grid[y_coords, x_coords] = Y\n",
        "    return grid\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    width, height = img.size\n",
        "    X, Y = train_data.tensors\n",
        "    plt.figure()\n",
        "    plt.imshow(to_grid(X, Y, width, height))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uep2eUdhMfhZ"
      },
      "source": [
        "\n",
        "## DataLoaders\n",
        "\n",
        "Today, our `Dataset` is small enough to fit in memory, so we could just use `torch.randperm` on our training set to fetch random batches from it.\n",
        "\n",
        "In general, we only want to load parts of our dataset as they're needed because our dataset may be too large to fit in memory, it may take too long to preprocess the entire dataset, or we may just want the GPU to be active as much as possible instead of waiting for data to be ready.\n",
        "\n",
        "This is where `torch.DataLoader` comes in. A `DataLoader` instance is responsible for spawning multiple worker processes which load data in parallel and communicate back to the `DataLoader`. Ideally, the `DataLoader` can prepare the next batch while the GPU is processing the current one, eliminating GPU downtime.\n",
        "\n",
        "We'll implement our own version of this another day when we're dealing with parallelism, and just use the PyTorch implementation today. We've provided DataLoaders with `shuffle=True` for the train loader. What would happen if you didn't shuffle the training data?\n",
        "\n",
        "<details>\n",
        "<summary>Answer - Shuffling Training Data</summary>\n",
        "\n",
        "If our training data was sorted and we didn't shuffle it at least once, then the learning process could oscillate instead of converging. Suppose that the top half of the image was mostly blue sky and the bottom half was mostly green grass. The model would get gradients that first suggest \"everything is mostly blue\" and later \"everything is mostly green\" successively. In this case, we already used `randperm` above so our training data has been shuffled regardless.\n",
        "\n",
        "In practice, SGD is relatively insensitive to whether you shuffle on every epoch, just once, or even sample each minibatch with replacement from the full dataset. For some theory behind this, see [this paper](https://arxiv.org/pdf/2106.06880.pdf).\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVpFUyn8MgWn"
      },
      "outputs": [],
      "source": [
        "if MAIN:\n",
        "    train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
        "    val_loader = DataLoader(val_data, batch_size=256)\n",
        "    test_loader = DataLoader(test_data, batch_size=256)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BeZD0nM6Mh5y"
      },
      "source": [
        "\n",
        "Implement the `train_one_epoch` function below.\n",
        "\n",
        "- Use the `to()` method of a `Tensor` to send the data to the device indicated by the global variable `device`.\n",
        "- You can convert a one-element tensor to a regular Python number using the `item` method.\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>It's not working and I'm confused!</summary>\n",
        "\n",
        "- Did you remember to call `optimizer.zero_grad()` before each forward pass?\n",
        "- Does `model.parameters()` return what you expect?\n",
        "- Are you calling `backward()` on the mean loss over the batch items? Note that if you don't use the mean, the magnitude of the gradients scales up linearly with the batch size, which is not what you want.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNJow6GuMi1L"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model: ImageMemorizer, dataloader: DataLoader) -> float:\n",
        "    \"\"\"Show each example in the dataloader to the model once.\n",
        "\n",
        "    Use `torch.optim.Adam` for the optimizer (you'll build your own Adam optimizer later today).\n",
        "    Use `F.l1_loss(prediction, actual)` for the loss function. This just puts less weight on very bright or dark pixels, which seems to produce nicer images.\n",
        "\n",
        "    Return: the average loss per example seen, i.e. sum of losses of each batch weighted by the size of the batch, divided by the total number of examples seen\n",
        "    \"\"\"\n",
        "    \n",
        "    ...\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    w1d4_part1_test.test_train(train_one_epoch)\n",
        "\n",
        "\n",
        "def evaluate(model: ImageMemorizer, dataloader: DataLoader) -> float:\n",
        "    \"\"\"Return the total L1 loss over the provided data divided by the number of examples.\"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()  # Does nothing on this particular model, but good practice to have it\n",
        "    with t.inference_mode():\n",
        "        sum_loss = 0.0\n",
        "        n_elems = 0\n",
        "        for X, y in dataloader:\n",
        "            sum_loss += F.l1_loss(model(X.to(device)), y.to(device)).item() * len(X)\n",
        "            n_elems += len(X)\n",
        "        return sum_loss / n_elems\n",
        "\n",
        "if MAIN:\n",
        "    w1d4_part1_test.test_evaluate(evaluate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_gbo8h4BMjq5"
      },
      "source": [
        "\n",
        "The following cell creates a model with 400 neurons in each hidden layer and trains it for an epoch.\n",
        "\n",
        "If no errors appeared, do a few more epochs and plot the training loss and validation loss over time as a function of number of epochs. Compute the validation loss using your `evaluate` function. I was able to reach a validation loss below 0.2 after 40 epochs. Your image might be easier or harder to learn.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwTqEyTPMkjC"
      },
      "outputs": [],
      "source": [
        "if MAIN:\n",
        "    model = ImageMemorizer(2, 400, 3)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    \n",
        "    num_epochs = 1\n",
        "    bar = tqdm(range(num_epochs))\n",
        "    for epoch in bar:\n",
        "        train_losses.append(train_one_epoch(model, train_loader))\n",
        "        val_loss = evaluate(model, val_loader)\n",
        "        bar.set_description(f\"val loss: {val_loss:.3f}\")\n",
        "        bar.refresh()\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(train_losses, label=\"Training loss\")\n",
        "    ax.plot(val_losses, label=\"Validation loss\")\n",
        "    ax.set(xlabel=\"Epochs\", ylabel=\"L1 Loss\")\n",
        "    fig.legend()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BmLlpGzAMljt"
      },
      "source": [
        "\n",
        "\n",
        "Finally, execute this cell to display the image your network has memorized:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVY5wOklMmdZ"
      },
      "outputs": [],
      "source": [
        "if MAIN:\n",
        "    X = all_coordinates_scaled(height, width)\n",
        "    with t.inference_mode():\n",
        "        Y = model(X.to(device)).cpu()\n",
        "    grid = to_grid(X, Y, width, height)\n",
        "    grid.clip_(0, 1)\n",
        "    fig, ax = plt.subplots(figsize=(12, 12))\n",
        "    ax.imshow(grid)\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # ax.set_position([0, 0, 1, 1])\n",
        "    # fig.savefig(\"w1d4_vangogh_solution.jpg\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UbwpxnGhMngU"
      },
      "source": [
        "\n",
        "Share your image with your friends if you like it! Here's the one my network learned:\n",
        "\n",
        "![Alt text](https://github.com/EffiSciencesResearch/ML4G/blob/main/mlab/w1d4_vangogh_solution.jpg?raw=true \"a title\")\n",
        "\n",
        "\n",
        "## Visualising Optimization With Rosenbrock's Banana\n",
        "\n",
        "\"Rosenbrock's Banana\" is a (relatively) famous function that has a simple equation but is challenging to optimize because of the shape of the loss landscape.\n",
        "\n",
        "Use `plot_rosenbrock` to plot the log of the function. Where is the minimum?\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Solution</summary>\n",
        "\n",
        "The first term is minimized when $x=a$ and the second term is minimized when $y = x^2 = a^2$. For $a=1$, it's $(1, 1)$. Looking at the plot, this seems reasonable.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>How do I generate the $x$ and $y$ values for my contour plot?</summary>\n",
        "\n",
        "For each dimension, use `torch.linspace` to generate evenly spaced values, then `einops.repeat`.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZsZtTB0Momi"
      },
      "outputs": [],
      "source": [
        "def rosenbrocks_banana(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
        "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
        "\n",
        "\n",
        "def plot_rosenbrock(xmin=-2, xmax=2, ymin=-1, ymax=3, n_points=50, log_scale=False) -> matplotlib.figure.Figure:\n",
        "    \"\"\"Plot the rosenbrocks_banana function over the specified domain.\n",
        "\n",
        "    If log_scale is True, take the logarithm of the output before plotting.\n",
        "    \"\"\"\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    x = t.linspace(xmin, xmax, n_points)\n",
        "    y = t.linspace(ymin, ymax, n_points)\n",
        "    xx = repeat(x, \"x -> y x\", y=n_points)\n",
        "    yy = repeat(y, \"y -> y x\", x=n_points)\n",
        "    zs = rosenbrocks_banana(xx, yy)\n",
        "    contour = ax.contourf(x, y, t.log(zs) if log_scale else zs)\n",
        "    ax.contour(contour)\n",
        "    ax.set(xlabel=\"x\", ylabel=\"y\")\n",
        "    return fig\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    plot_rosenbrock()\n",
        "    fig = plot_rosenbrock(log_scale=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bThJXfzKMpad"
      },
      "source": [
        "\n",
        "# Optimize The Banana\n",
        "\n",
        "Implement the `opt_banana` function using `torch.optim.SGD`. Starting from `(-1.5, 2.5)`, run your function and add the resulting trajectory of `(x, y)` pairs to your contour plot. Did it find the minimum? Play with the learning rate and momentum a bit and see how close you can get within 100 iterations.\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>I'm not sure if my `opt_banana` is implemented properly.</summary>\n",
        "\n",
        "With a learning rate of `0.001` and momentum of `0.98`, my SGD was able to reach `[ 1.0234299 ,  1.198282 ]` after 100 iterations.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>I'm getting \"Can't call numpy() on Tensor that requires grad\" and I don't know why!</summary>\n",
        "\n",
        "This is a protective mechanism built into PyTorch. The idea is that once you convert your `Tensor` to NumPy, PyTorch can no longer track gradients, but you might not understand this and expect backprop to work on NumPy arrays.\n",
        "\n",
        "All you need to do to convince PyTorch you're a responsible adult is to call `detach()` on the tensor first, which returns a view that does not require grad and isn't part of the computation graph.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA08_PkSMqOl"
      },
      "outputs": [],
      "source": [
        "def opt_banana(xy: t.Tensor, n_iters: int, lr=0.001, momentum=0.98):\n",
        "    \"\"\"Optimize the banana starting from the specified point.\n",
        "\n",
        "    xy: shape (2,). The (x, y) starting point.\n",
        "    n_iters: number of steps.\n",
        "\n",
        "    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n",
        "    \"\"\"\n",
        "    assert xy.requires_grad\n",
        "    \n",
        "    ...\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    xy = t.tensor([-1.5, 2.5], requires_grad=True)\n",
        "    xys = opt_banana(xy, n_iters=100).numpy()\n",
        "    fig = plot_rosenbrock(log_scale=True)\n",
        "    fig.axes[0].plot(xys[:, 0], xys[:, 1], color=\"r\", linewidth=1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Eh2ij1MrDb"
      },
      "source": [
        "\n",
        "## Build Your Own Optimizers\n",
        "\n",
        "Now let's build our own drop-in replacement for these three classes from `torch.optim`. The documentation pages for these algorithms have pseudocode you can use to implement your step method.\n",
        "\n",
        "First Read this article:\n",
        "https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c\n",
        "\n",
        "### Gotcha: In-Place Operations\n",
        "\n",
        "Be careful with expressions like `x = x + y` and `x += y`. They are NOT equivalent in Python.\n",
        "\n",
        "- The first one allocates a new `Tensor` of the appropriate size and adds `x` and `y` to it, then rebinds `x` to point to the new variable. The original `x` is not modified.\n",
        "- The second one modifies the storage referred to by `x` to contain the sum of `x` and `y` - it is an \"in-place\" operation.\n",
        "  - Another way to write the in-place operation is `x.add_(y)` (the trailing underscore indicates an in-place operation).\n",
        "  - A third way to write the in-place operation is `torch.add(x, y, out=x)`.\n",
        "- This is rather subtle, so make sure you are clear on the difference. This isn't specific to PyTorch; the built-in Python `list` follows similar behavior: `x = x + y` allocates a new list, while `x += y` is equivalent to `x.extend(y)`.\n",
        "- In general, the first version calls the method `x.__add__(y)` while the second calls `x.__iadd__(y)`, and these two methods can have arbitrary semantics.\n",
        "\n",
        "The tricky thing that happens here is that both the optimizer and the `Module` in your model have a reference to the same `Parameter` instance. Do we want to use in-place operations in our optimizer?\n",
        "\n",
        "<details>\n",
        "\n",
        "<summary>Solution - In-place Operations</summary>\n",
        "\n",
        "You MUST use in-place operations in your optimizer because we want the model to see the change to the Parameter's storage on the next forward pass. If your optimizer allocates a new tensor, the model won't know anything about the new tensor and will continue to use the old, unmodified version.\n",
        "\n",
        "</details>\n",
        "\n",
        "### More Tips\n",
        "\n",
        "- The provided `params` might be a generator, in which case you can only iterate over it once before the generator is exhausted. Copy it into a `list` to be able to iterate over it repeatedly.\n",
        "- Your step function shouldn't modify the gradients. Use the `with torch.inference_mode():` context for this. Fun fact: you can instead use `@torch.inference_mode()` (note the preceding `@`) as a method decorator to do the same thing.\n",
        "- If you create any new tensors, they should be on the same device as the corresponding parameter. Use `torch.zeros_like()` or similar for this.\n",
        "- Be careful not to mix up `Parameter` and `Tensor` types in this step.\n",
        "- The actual PyTorch implementations have an additional feature called parameter groups where you can specify different hyperparameters for each group of parameters. You can ignore this for today.\n",
        "\n",
        "Tip: It is possible to not use if conditions in sgd and Adam. This simplifies the code.\n",
        "\n",
        "### SGD\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Why do we have to zero the gradient in PyTorch? \n",
        "- Why do we use the word 'stochastic' in 'Stochastic gradient descent' in the context of deep learning?\n",
        "\n",
        "SGD:\n",
        "- Below, read the method zero_grad. You can note that in PyTorch, to zero a gradient means assigning None.\n",
        "- We will implement successively the pure SGD, the sgd with weight decay, and the SGD with momentum.\n",
        "\n",
        "Implement steps:\n",
        "1. `# update` Implement the most basic version of SGD possible\n",
        "- Why do we need the 'with torch.inference_mode()' context manager?\n",
        "2. `# weight_decay`: What is the formula of the update when there is some weight_decay (ie when we penalize each parameter squared)? Assume wd absorbs the constant.\n",
        "- Tip: let's say we optimize `L(X, y) = (ax_1 + bx_2 + c - y)^2` with respect to `a`, `b` and `c`.\n",
        "- Adding weight_decay means that instead of minimizing `L`, we minimize `g(X, y) =  L(X, y) + wd(a^2 + b^2 + c^2)/2`\n",
        "- For this example, what is the formula of the gradient wrt `a`,`b` and `c`?\n",
        "- In the code, at the beginning of the step function, replace the gradient by `g = p.grad + self.wd * p`\n",
        "3. `# momentum`: Why do we need `self.running_average` in the `__init__`? Add momentum. Separate the cases `self.momentum` equals zero or not.\n",
        "\n",
        "There are multiple ways to implement SGD, so don't panic if there is some ambiguity and look at the solution to compare with the PyTorch implementation when you have used every variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETUIC33SMsPi"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    def __init__(self, params: Iterable[t.nn.parameter.Parameter], lr: float, momentum: float, weight_decay: float):\n",
        "        \"\"\"Implements SGD with momentum.\n",
        "\n",
        "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.wd = weight_decay\n",
        "        self.momentum = momentum\n",
        "        \n",
        "        # running average of the gradient if there is some momentum\n",
        "        self.running_average = [None for _ in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for p in self.params:\n",
        "            p.grad = None\n",
        "\n",
        "    def step(self) -> None:\n",
        "        with t.inference_mode():\n",
        "            for i, p in enumerate(self.params):\n",
        "                # weight decay\n",
        "                ...\n",
        "\n",
        "                # momentum\n",
        "                ...\n",
        "\n",
        "                # update\n",
        "                ...\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    w1d4_part1_test.test_sgd(SGD)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kaF8bVdaMtEc"
      },
      "source": [
        "\n",
        "### Adam\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adam, by far the most used optimizer.\n",
        "\n",
        "It's a combination of SGD+RMSProps and uses one momentum for the gradient, and another for the gradient squared.\n",
        "\n",
        "1. Adam\n",
        "Tip: There is no `if` condition in Adam because `beta1` and `beta2` are always stricly positive.\n",
        "- `sum_of_gradient = previous_sum_of_gradient * beta1 + gradient * (1 - beta1)` (SGD+Momentum)\n",
        "- `sum_of_gradient_squared = previous_sum_of_gradient_squared * beta2 + gradient² * (1- beta2)` (RMSProp)\n",
        "- `delta = -learning_rate * sum_of_gradient / sqrt(sum_of_gradient_squared)`\n",
        "- Update the gradient\n",
        "\n",
        "2. More stability + regularization\n",
        "- Add `self.eps` to the denominator, outside the square root.\n",
        "- At the beginning of the step function, replace the gradient by `g = p.grad + self.wd * p`\n",
        "\n",
        "3. Adam + Correction\n",
        "- In the `__init__`, `self.b1` and `self.b2` are a list of zeros, so we need a correction:\n",
        "- In the update, use a correction: `b1_hat = self.b1[i] / (1.0 - self.beta1**self.t)`\n",
        "- Generally `beta1 = 0.9`, and `beta2 = 0.999`.\n",
        "- Same for `b2_hat`.\n",
        "\n",
        "\n",
        "NB: The correction is important only at the beginning of the optimization process (self.t small).\n",
        "The the correction is needed because self.b1 and self.b2 are initialized at zero.\n",
        "\n",
        "Try to pass the tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtwzqBuvMuC8"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.001,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        \"\"\"Implements Adam.\n",
        "\n",
        "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1, self.beta2 = betas  # momenti of b1 and b2\n",
        "        self.eps = eps\n",
        "        self.wd = weight_decay\n",
        "\n",
        "        # sum_of_gradient for each param\n",
        "        # & sum_of_gradient_squared for each param\n",
        "        self.b1 = [torch.zeros_like(p) for p in self.params]\n",
        "        self.b2 = [torch.zeros_like(p) for p in self.params]\n",
        "        self.t = 0\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        ...\n",
        "\n",
        "    def step(self) -> None:\n",
        "        ...\n",
        "\n",
        "\n",
        "if MAIN:\n",
        "    w1d4_part1_test.test_adam(Adam)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML4GS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.16 (main, Jan 11 2023, 10:02:19) \n[Clang 14.0.6 ]"
    },
    "vscode": {
      "interpreter": {
        "hash": "2976a061e207041c5ea034acf520ab7eefe3f1128fd85ff62a2d139d31ccd0fd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
